# HandSpeak

## ğŸ“Œ Overview

This project uses MediaPipe, OpenCV, and a deep learning model to recognize hand gestures and sign language. The system can detect both numbers and words based on hand movements captured via a webcam. It processes a sequence of frames, extracts key points from hand landmarks, and predicts the corresponding sign.

## ğŸ› ï¸ Features
- ğŸ“· **Real-time webcam feed processing**
- âœ‹ **Detects numbers and words from hand signs**
- ğŸ¯ **Ensures one of the hands are detected for accurate predictions**
- ğŸ“Š **Filters out low-confidence predictions to reduce errors**


## ğŸ‘¥ Who Can Use This?

   This project can be used by:

   ğŸ« Students & Researchers working on AI-based gesture recognition.

  ğŸ§â€â™‚ï¸ Deaf & Hard of Hearing Individuals to communicate via sign language.

  ğŸ“± Developers & Engineers to integrate sign recognition into applications.
  
  ğŸ“ Educators & Institutions for learning and teaching sign language.

## âœ… How This Project is Useful
- Bridges communication gaps between hearing-impaired and non-signers.
- Automates sign language recognition, making it easier for accessibility solutions.
- Enhances learning experiences by providing real-time feedback on sign gestures.
- Can be integrated with AI assistants for hands-free communication.

## ğŸš€ Getting Started
ğŸ“¦ Requirements
- Prerequisites
- Python 3.x
- Jupyter
- OpenCV
- MediaPipe
- TensorFlow/Keras
- NumPy

## ğŸ›  Steps to Run
### 1ï¸âƒ£ Install Dependencies
Ensure you have **Python 3.7+** installed, then run:
```bash
pip install numpy opencv-python mediapipe tensorflow
```

### 2ï¸âƒ£ Clone the Repository
```bash
git clone git@github.com:Amatullagajipurwala/HandSpeak.git
```

###  3ï¸âƒ£ Run Project
```bash
jupyter notebook
```

### 4ï¸âƒ£ Controls
- Press **'Q'** to **exit** the application.

## ğŸ¯ How It Works
1. **Captures Webcam Input** ğŸ¥
   - Reads frames continuously from the webcam.
2. **Extracts Hand Keypoints** ğŸ–
   - Uses **MediaPipe Holistic** to track hand landmarks.
3. **Predicts the Sign** ğŸ”¤
   - Uses a **pretrained model** to classify the sign (number/word).
4. **Displays the Word on Screen** ğŸ–¥
   - If confidence is high, the detected word is displayed.
5. **Ignores Incorrect or No-Hand Cases** ğŸš«
   - If both hands are not detected, the display remains empty.

## ğŸ† Future Enhancements
- âœ… **Add more signs & numbers to the dataset**
- âœ… **Optimize detection speed**
- âœ… **Enhance accuracy using more training data**

## ğŸ¤ Contributors
ğŸ‘¤ **Srushti** â€“ [GitHub Profile](https://github.com/Srushti906)

ğŸ‘¤ **Nensee** â€“ [GitHub Profile](https://github.com/NenseeZankat)

ğŸ‘¤ **Disha** â€“ [GitHub Profile](https://github.com/PatelDishaJ)

## ğŸ“œ License
ğŸ“ This project is **open-source** and licensed under the MIT License.



ğŸ“¢ If you like this project, don't forget to star â­ it on GitHub!



